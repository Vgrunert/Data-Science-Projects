% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Introduction to Kalman-Filter},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{Introduction to Kalman-Filter}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Bayesian filtering provides a framework to solve linear and non-linear
stochastic systems. Due to their flexible nature, bayesian filtering
methods are being implemented in various fields such as economics,
robotics and biology amongst others. Using the baysian approach allows
the estimation problem to be solved in a recursive manner. Consequently,
a model is able to update it's parameters with each new observation. The
ability to monitor the developement of the state of a system is a key
advantage of bayesian filtering methods over other methods such as
classical linear regression.

Bayesian filtering refers to a broad class of methods. There are several
different bayesian filtering methods which can broadly be devided into
gaussian and non gaussian methods. A model is classified as gaussian,
when any finite number of linear combinations of random variables
follows a multivariate normal distribution. Here we considere two
gaussian methods, the Kalman-Filter, the Extended-Kalman-Filter and the
Particle-Filter, which is a non gaussian method.

Bayesian filters originate in optimal control theory. Thus historically
the control theory notation has been used to describe Bayesian filtering
models, which is divergent from other fields. The above mentioned models
are generally referred to as state-space models. The state is set of
unknown parameters of which the system depends, and the state-space is
the set of all possible values the state can have. The model describes
the relationship between unobserved states and observed measurements.

This presentation is structured as following: section 2 presents the key
derivations of the state-space model equations. Section three revisits
matrix algebra followed and section four the properties of the
multivariate gaussian distribution. The matrix algebra and the
multivariate gaussian distribution are essential in deriving the
Kalman-Filter equations, which are presented in section five. In section
six the key equations and their derivations of the
Extended-Kalman-Filter are presented. Finally, section seven presents an
introduction to the Particel-Filter. The paper ends with a short
conclusion that summarizes the main ideas of this paper.

\hypertarget{the-state-space-model}{%
\section{The state-space model}\label{the-state-space-model}}

\begin{equation*} 
    \begin{aligned}
        \textbf{x}_t &\sim p(\textbf{x}_t|\textbf{x}_{t-1}) \\
        \textbf{y}_t &\sim p(\textbf{y}_t|\textbf{x}_t)
    \end{aligned}    
\end{equation*}

where the state \(\textbf{x}_t\) is assumed to have the Markov property,
meaning that no other dependencies than the ones specified above exist.
The variable \(\textbf{y}_t\) is the variable of interest, also referred
to as the observation or measurement. For the purpose of conducting
inference on the state using measurements, the quantity of interest is
the posterior distribution if the current state, given all measurements
\(\textbf{y}_{1:t}\):

\begin{equation*} 
    p(\textbf{x}_t|\textbf{y}_{1:t}) 
\end{equation*}

The process of calculating these quantities begins with the assumption
that the previous value \(p(\textbf{x}_{t-1}|\textbf{y}_{t-1})\) is
known. Then using the fact that:

\begin{equation*} 
    \begin{aligned}
        p(\textbf{x}_{t}, \textbf{x}_{t-1}|\textbf{y}_{1:t-1}) &= p(\textbf{x}_{t}|\textbf{y}_{1:t-1}, \textbf{x}_{t-1}) p(\textbf{x}_{t-1} |\textbf{y}_{1:t-1}) \\
                                                               &= p(\textbf{x}_{t}|\textbf{x}_{t-1}) p(\textbf{x}_{t-1} |\textbf{y}_{1:t-1})                               
    \end{aligned}
\end{equation*}

where
\(p(\textbf{x}_{t}|\textbf{y}_{1:t-1}, \textbf{x}_{t-1}) = p(\textbf{x}_{t}|\textbf{x}_{t-1})\)
by assuming the Markov property holds. This equality decomposes the
problem of calculating the probability of observing past and current
state conditional on previous measurements into a part which is assumed
to be known \(p(\textbf{x}_{t-1} |\textbf{y}_{1:t-1})\) and one for
which there is a model \(p(\textbf{x}_{t}|\textbf{x}_{t-1})\). The next
step is called prediction and consists of calculating:

\begin{equation*} 
    \begin{aligned}
        p(\textbf{x}_{t}| \textbf{y}_{1:t-1}) &= \int p(\textbf{x}_{t}, \textbf{x}_{t-1}| \textbf{y}_{1:t-1}) \textbf{dx}_{t-1} \\
                                              &= \int p(\textbf{x}_{t}|\textbf{x}_{t-1}) p(\textbf{x}_{t-1} |\textbf{y}_{1:t-1}) \textbf{dx}_{t-1}
    \end{aligned}
\end{equation*}

because
\(p(\textbf{x}_{t}, \textbf{x}_{t-1}|\textbf{y}_{1:t-1}) = p(\textbf{x}_{t}|\textbf{x}_{t-1}) p(\textbf{x}_{t-1} |\textbf{y}_{1:t-1})\)
(Kolmogorov-Chapman equation) and the fact for two random variables
\(X,Y\) and for an arbitrary density function \(f_{X,Y}\):
\begin{equation*}
    f_X(x) = \int f_{X,Y}(x,y) dy
\end{equation*}

With new measurements \(\textbf{y}_t\) being observed, using the Bayes'
theorem and the fact that
\(p(\textbf{y}_{1:t}) = p(\textbf{y}_t, \textbf{y}_{1:t-1})\) the
quantity \(p(\textbf{x}_t|\textbf{y}_{1:t})\) and can be reformulated
as:

\begin{equation*} 
    \begin{aligned}
        p(\textbf{x}_t|\textbf{y}_{1:t}) &=  p(\textbf{x}_t|\textbf{y}_{t}, \textbf{y}_{1:t-1}) \\
                                         &=  \frac{p(\textbf{y}_{t}|\textbf{x}_t, \textbf{y}_{1:t-1}) p(\textbf{x}_t| \textbf{y}_{1:t-1}) }{p(\textbf{y}_{t}|\textbf{y}_{1:t-1})} \\
                                         &= \frac{p(\textbf{y}_{t}|\textbf{x}_t) p(\textbf{x}_t| \textbf{y}_{1:t-1}) }{\int p(\textbf{y}_{t}|\textbf{x}_{t})p(\textbf{x}_t
                                         |\textbf{y}_{1:t-1}) \textbf{dx}_t} \\
    \end{aligned}
\end{equation*}

\hypertarget{matrix-algebra}{%
\section{Matrix Algebra}\label{matrix-algebra}}

\hypertarget{matrix-inversion}{%
\subsection{Matrix Inversion}\label{matrix-inversion}}

This section presents matrix lemmas that are helpful in deriving the
equations for the Kalman-Filter equations. Let
\(A \in \mathbb{R}^{n \times n}\), \(D \in \mathbb{R}^{m \times m}\),
\(B \in \mathbb{R}^{n \times m}\) and \(C \in \mathbb{R}^{m \times n}\)
and:

\[
\begin{equation*}
        M = \begin{pmatrix}
            A & B \\ 
            C & D \\ 
        \end{pmatrix}, 
        M^{-1} = \begin{pmatrix}
            E & F \\ 
            G & H \\ 
        \end{pmatrix}
\end{equation*}
\]

such that:

\[
\begin{equation*} 
    \begin{pmatrix}
        A & B \\ 
        C & D \\ 
    \end{pmatrix} 
    \begin{pmatrix}
        E & F \\ 
        G & H \\ 
    \end{pmatrix} = 
    \begin{pmatrix}
        I_n & O \\ 
        O & I_m \\ 
    \end{pmatrix} 
\end{equation*}
\]

then assuming \(A\) and \(D\) are invertible: \[
\begin{equation*} 
    \begin{aligned}
        E &= (A - BD^{-1}C)^{-1} \\
        H &= (D - CA^{-1}B)^{-1} \\
    \end{aligned}
\end{equation*}
\]

and

\[
\begin{equation*}
    \begin{aligned}
        F &= - (A - BD^{-1}C)^{-1}BD^{-1} = - A^{-1}B(D - CA^{-1}B)^{-1} \\
        G &= - (D - CA^{-1}B)^{-1}CA^{-1} = - D^{-1}C(A - BD^{-1}C)^{-1} \\
    \end{aligned}
\end{equation*}
\]

and

\[
\begin{equation*}
    \begin{aligned}
        (A - BD^{-1}C)^{-1} &= A^{-1} + A^{-1}B(D - CA^{-1}B)^{-1}CA^{-1} \\
        (D - CA^{-1}B)^{-1} &= D^{-1} + D^{-1}C(A - BD^{-1}C)^{-1}BD^{-1} \\
    \end{aligned}
\end{equation*}
\] \#\# Matrix Decomposition Given the previous assumptions: \[
\begin{equation*}
    \begin{aligned} M = 
        \begin{pmatrix}
            A & B \\ 
            C & D \\ 
        \end{pmatrix} &= 
        \begin{pmatrix}
            I & B \\ 
            O & D \\ 
        \end{pmatrix} \begin{pmatrix}
            A - BD^{-1}C & O \\ 
            D^{-1}C & I \\             
        \end{pmatrix} \\
        &= \begin{pmatrix}
            A & O \\ 
            C & I \\ 
        \end{pmatrix} \begin{pmatrix}
            I & A^{-1}B \\ 
            O & D - CA^{-1}B \\             
        \end{pmatrix} 
    \end{aligned}
\end{equation*}
\]

It follows that: \[
\begin{equation*}
    \begin{aligned}
    |det(M)|  &= |det(D)||det(A - BD^{-1}C)| \\
                &= |det(A)||det(D - CA^{-1}B)|
    \end{aligned}
\end{equation*}
\] \# Gaussian Distribution

In the following key properties of the gaussian distribution will be
presented.

A random variable \(Z \in \mathbb{R}\) is following a gaussian
distribution if, its density is defined as: \[
\begin{equation*}
    f_Z(z) = \frac{1}{\sqrt{2\pi}} exp(-\frac{z^2}{2})
\end{equation*}
\]

and the joint density of identical, independently distributed (iid)
gaussians is: \[
\begin{equation*} 
    \begin{aligned}
        f_{Z_1,...,Z_n}(z_1,...,z_n) &= \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} exp(-\frac{z_i^2}{2}) \\
                      &= (2\pi)^{-\frac{n}{2}}exp(-\frac{\sum_{i=1}^nz^2}{2}) \\
                      &= (2\pi)^{-\frac{n}{2}}exp(-\frac{\textbf{(z - 0)}^T \textbf{I}_n \textbf{(z - 0)}}{2}) 
    \end{aligned}
\end{equation*}
\]

which is the product density and thus itself a density (without proof)
and will be referred to as a multivariate gaussian distribution with
\(\textbf{0}\) mean and \(I_n\) as the covariance matrix, in short:

\[
\begin{equation*} 
        \textbf{Z} = (Z_1,...,Z_n) \sim \mathcal{N}(\textbf{0}, \textbf{I}_n)
\end{equation*}
\]

The multivariate gaussian has the property that linear transformations
follow again a multivariate gaussian distribution. The proof of this
theorem requires the transformation theorem for integrals and the
diagonalization of symmetric matrices theorem.

If \(\Sigma \in \mathbb{R}^{n\times n}\), \(\Sigma = \Sigma^T\),
\(Z = (Z_1,..., Z_n) \sim \mathcal{N}(\textbf{0}, \textbf{I}_n)\),
\(\mu = (\mu_1,..., \mu_n), \mu_i \in \mathbb{R}\) and
\(\textbf{X} = \Sigma^{\frac{1}{2}} \textbf{Z} + \mu\) then: \[
\begin{equation*}
    \textbf{X} \sim \mathcal{N}(\mu, \Sigma)
\end{equation*} 
\]

Thus \(\textbf{X}\) follows a multivariate gaussian distribution with
mean \(\mu\) and covariance \(\Sigma\).

Following the exact same reasoning and under the same assumptions as
previously and if \(A \in \mathbb{R}^{k \times n}\) with \(AA^{-1} = I\)
then:

\[
\begin{equation*}
    \textbf{Y} = A\textbf{X} + b \sim \mathcal{N}(A\mu, A\Sigma A^T)
\end{equation*}
\]

This equation will be referred to as the reproductive property of the
multivariate gaussian distribution. The key takeaway from this section
is that linear transformations of multivariate gaussian distributions
are themselves multivariate gaussian distributed.

Under the same assumptions as in Lemma 4.1 and if
\(X \sim \mathcal{N}(\mu_X, \Sigma_X)\) and
\(Y \sim \mathcal{N}(\mu_Y, \Sigma_Y)\), then: \[
\begin{equation*} 
    \begin{aligned} 
        X|Y \sim \mathcal{N}(\mu_x + \Sigma_{X,Y}\Sigma_Y^{-1}(\textbf{y}-\mu_Y), (\Sigma_X - \Sigma_{X,Y}\Sigma_Y^{-1}\Sigma_{Y,X})) \\
        Y|X \sim \mathcal{N}(\mu_Y + \Sigma_{X,Y}^T\Sigma_X^{-1}(\textbf{x}-\mu_X), (\Sigma_Y - \Sigma_{Y,X}\Sigma_X^{-1}\Sigma_{X,Y})) 
    \end{aligned}
\end{equation*}
\] These distribution can be derived from the matrix inversion lemma and
the Woodbury formula. These conditional posterior distributions are the
basis for the Kalman-Filter.

\hypertarget{kalman-filter}{%
\section{Kalman Filter}\label{kalman-filter}}

The Kalman-Filter is a method to estimate dynamic linear systems with
gaussian noise. Gaussian noise can be additive or take other functional
forms. In this analysis we are only considering the case for additive
noise. The system is assumed to take the following form:

\[
\begin{equation*} 
    \begin{aligned}
        \textbf{x}_t &= \textbf{A}_{t-1} \textbf{x}_{t-1} + \textbf{q}_{t-1},\ \textbf{q}_{t-1} \sim \mathcal{N}(\textbf{0},\textbf{Q}_{t-1}) \\
        \textbf{y}_t &= \textbf{H}_t\textbf{x}_t + \textbf{r}_t, \ \textbf{r}_t \sim \mathcal{N}(\textbf{0},\textbf{R}_{t})          
    \end{aligned}
\end{equation*}
\]

where \(\textbf{x}_t \in \mathbb{R}^n\) is the state and
\(\textbf{y}_t \in \mathbb{R}^m\) is the measurement. This equation
describes the state and process dynamics. The transition between the
current and past state is captured through \(\textbf{A}_{t-1}\) and the
relation between the current state and the measurement is captured by
\(\textbf{H}_t\), where \(\textbf{r}_t\) is the measurement noise. Both
\(q_{t-1}\) and \(r_t\) are assumed to be independent.

\hypertarget{prediction-step}{%
\subsection{Prediction Step}\label{prediction-step}}

For the purpose of solving the baysian filtering problem, we are
interested in calculating first \(p(\textbf{x}_t|\textbf{y}_{1:t-1})\),
which we can calculate directly using the reproductive property of the
gaussian distribution. Assuming:
\(\textbf{x}_{t-1}|\textbf{y}_{1:t-1} \sim \mathcal{N}(\mu_{x_{t-1}}, \Sigma_{x_{t-1}} )\):

\[
\begin{equation*}
    \begin{aligned}
        \textbf{m}_t &:=\textbf{A}_{t-1}\mu_{x_{t-1}} \\
        \textbf{C}_t &:= \textbf{A}_{t-1}\Sigma_{x_{t-1}}\textbf{A}_{t-1}^T + \textbf{Q}_{t-1} \\
\end{aligned}
\end{equation*}
\]

then: \[
\begin{equation}
    \begin{aligned}
        \textbf{x}_t|\textbf{y}_{1:t-1}  &\sim \mathcal{N}(\textbf{m}_t, \textbf{C}_t) \\
\end{aligned}
\end{equation}
\]

This step is also referred to as the ``prediction step''.

\hypertarget{update}{%
\subsection{Update}\label{update}}

Next we calculate \(p(\textbf{y}_t|\textbf{x}_t, \textbf{y}_{1:t-1})\)
by applying the reproductive property again and obtain:

\[
\begin{equation}
    \begin{aligned}
        \textbf{y}_t|\textbf{x}_t, \textbf{y}_{1:t-1} &\sim \mathcal{N}(\textbf{H}_{t}\textbf{m}_t, \textbf{H}_t\textbf{C}_t\textbf{H}_t^T + \textbf{R}_{t}) \\
    \end{aligned}
\end{equation}
\]

To calculate \(p(\textbf{x}_t|\textbf{y}_{1:t})\), we first calculate
the joint distribution
\(p(\textbf{x}_t, \textbf{y}_t|\textbf{y}_{1:t-1})\):

\[
\begin{equation} 
    \begin{aligned} 
        \textbf{x}_t, \textbf{y}_t|\textbf{y}_{1:t-1} &\sim \mathcal{N}
        \begin{pmatrix}
            \begin{pmatrix}
                \textbf{m}_t \\
                \textbf{H}_{t}\textbf{m}_t
            \end{pmatrix}, 
            \begin{pmatrix}
                \textbf{C}_t & \textbf{C}_t\textbf{H}_{t}^T \\
                \textbf{H}_{t}\textbf{C}_t^T & \textbf{H}_t\textbf{C}_t\textbf{H}_t^T + \textbf{R}_{t}
            \end{pmatrix}
        \end{pmatrix}
    \end{aligned}
\end{equation}
\]

Where: \[
\begin{align*}
    cov(\textbf{x}_t, \textbf{y}_t|\textbf{y}_{1:t-1}) &= cov(\textbf{x}_t, \textbf{H}_t\textbf{x}_t + r_t|\textbf{y}_{1:t-1}) \\
                                                       &= cov(\textbf{x}_t, \textbf{H}_t\textbf{x}_t|\textbf{y}_{1:t-1}) + cov(\textbf{x}_t + r_t|\textbf{y}_{1:t-1}) \\
                                                       &= cov(\textbf{x}_t, \textbf{H}_t\textbf{x}_t|\textbf{y}_{1:t-1}) \\
                                                       &= \textbf{C}_t\textbf{H}_{t}^T
\end{align*}
\]

where \(cov(\textbf{x}_t + r_t|\textbf{y}_{1:t-1}) = 0\) due to the
independence between \(\textbf{x}_t\) and \(r_t\). Then we obtain:

\[
\begin{align*}
    \mu_t& := \textbf{m}_t + \textbf{C}_t\textbf{H}_{t}^T(\textbf{H}_t\textbf{C}_t\textbf{H}_t^T + \textbf{R}_{t})^{-1}(\textbf{y}_t - \textbf{H}_{t}\textbf{m}_t) \\
    \Sigma_t&:= (\textbf{C}_t - \textbf{C}_t\textbf{H}_{t}^T(\textbf{H}_t\textbf{C}_t\textbf{H}_t^T + \textbf{R}_{t})^{-1}\textbf{H}_{t} \\
\end{align*}    
\] and thus:

\[
\begin{equation}
    \textbf{x}_t|\textbf{y}_t, \textbf{y}_{1:t-1} \sim \mathcal{N}(\mu_t, \Sigma_t) \\
\end{equation}
\]

Thus we obtained a closed form solution for the linear dynamic
stochastic model.

\hypertarget{algorithm}{%
\subsection{Algorithm}\label{algorithm}}

To summarize the algorithm:

\begin{itemize}
\tightlist
\item
  predict:

  \begin{itemize}
  \tightlist
  \item
    \(\textbf{m}_t =\textbf{A}_{t-1}\mu_{x_{t-1}}\)
  \item
    \(\textbf{C}_t = \textbf{A}_{t-1}\Sigma_{x_{t-1}}\textbf{A}_{t-1}^T + \textbf{Q}_{t-1}\)
  \end{itemize}
\item
  update:

  \begin{itemize}
  \tightlist
  \item
    \(\nu_t = \textbf{y}_t - \textbf{H}_{t}\textbf{m}_t\)
  \item
    \(\textbf{S}_t = \textbf{H}_t\textbf{C}_t\textbf{H}_t^T + \textbf{R}_{t}\)
  \item
    \(\textbf{K}_t = \textbf{C}_t\textbf{H}_{t}^T\textbf{S}_t^{-1}\)
  \item
    \(\mu_t = \textbf{m}_t + \textbf{K}_t\nu_t\)
  \item
    \(\Sigma_t = \textbf{C}_t-\textbf{K}_t\textbf{S}_t\textbf{K}_t^T\)
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generating samples from the multivariate normal distribution }
\NormalTok{mvnorm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(n, mu, covar)\{}
    \KeywordTok{return}\NormalTok{((}\KeywordTok{t}\NormalTok{(}\KeywordTok{chol}\NormalTok{(covar)) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)) }\OperatorTok{+}\StringTok{ }\NormalTok{mu)}
\NormalTok{\}}
\CommentTok{#####}

\CommentTok{#####}
\CommentTok{# kalman filter equations inplementation}
\CommentTok{# the implementation is slightly different for vector inputs then matrix inputs}
\CommentTok{# which is why it requires two different "inner" implementations}
\NormalTok{kalman_filter <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(measurements, stateTransition, processNoise,}
\NormalTok{    outputMatrix, measurementNoise)\{    }

    \CommentTok{# vector input }
    \ControlFlowTok{if}\NormalTok{(}\KeywordTok{is.vector}\NormalTok{(measurements))\{}

        \CommentTok{# number of measurements}
\NormalTok{        n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(measurements)}

        \CommentTok{# number of states}
\NormalTok{        n_state <-}\StringTok{ }\KeywordTok{ncol}\NormalTok{(stateTransition)}

        \CommentTok{# init state matrix}
\NormalTok{        state <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{n_state, }\DataTypeTok{nrow=}\NormalTok{n)}

        \CommentTok{# use first measurement as init state}
\NormalTok{        state[}\DecValTok{1}\NormalTok{, ] <-}\StringTok{ }\KeywordTok{c}\NormalTok{(measurements[}\DecValTok{1}\NormalTok{], }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n_state }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{))}

        \CommentTok{# init state covariance matrix}
\NormalTok{        S <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{10}\NormalTok{, n_state))}

        \CommentTok{# kalman filter equations }
        \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\NormalTok{n)\{}
            \CommentTok{# predict}
\NormalTok{            m_ <-}\StringTok{ }\NormalTok{stateTransition }\OperatorTok{%*%}\StringTok{ }\NormalTok{state[i}\DecValTok{-1}\NormalTok{, ]}
\NormalTok{            S_ <-}\StringTok{ }\NormalTok{stateTransition }\OperatorTok{%*%}\StringTok{ }\NormalTok{S }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(stateTransition) }\OperatorTok{+}\StringTok{ }\NormalTok{processNoise}

            \CommentTok{# update}
\NormalTok{            S <-}\StringTok{ }\NormalTok{outputMatrix }\OperatorTok{%*%}\StringTok{ }\NormalTok{S_ }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(outputMatrix) }\OperatorTok{+}\StringTok{ }\NormalTok{measurementNoise}
\NormalTok{            K <-}\StringTok{ }\NormalTok{S_ }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(outputMatrix) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{solve}\NormalTok{(S)}
\NormalTok{            m <-}\StringTok{ }\NormalTok{m_ }\OperatorTok{+}\StringTok{ }\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{(measurements[i] }\OperatorTok{-}\StringTok{ }\NormalTok{outputMatrix }\OperatorTok{%*%}\StringTok{ }\NormalTok{m_)}
\NormalTok{            S <-}\StringTok{ }\NormalTok{S_ }\OperatorTok{-}\StringTok{ }\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{S }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(K)}

            \CommentTok{# save}
\NormalTok{            state[i,] <-}\StringTok{ }\NormalTok{m}
\NormalTok{        \}}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{(}\KeywordTok{is.matrix}\NormalTok{(measurements))\{}

        \CommentTok{# number of measurements}
\NormalTok{        n_measure <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(measurements)}

        \CommentTok{# number of states}
\NormalTok{        n_state <-}\StringTok{ }\KeywordTok{ncol}\NormalTok{(stateTransition)}

        \CommentTok{# init state }
\NormalTok{        state <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{n_state, }\DataTypeTok{nrow=}\NormalTok{n_measure)}

        \CommentTok{# use first measurement as init state}
\NormalTok{        state[}\DecValTok{1}\NormalTok{, ] <-}\StringTok{ }\KeywordTok{c}\NormalTok{(measurements[}\DecValTok{1}\NormalTok{,], }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n_state }\OperatorTok{-}\StringTok{ }\KeywordTok{ncol}\NormalTok{(measurements)))}

        \CommentTok{# state covariance matrix}
\NormalTok{        S <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{10}\NormalTok{, n_state))}

        \CommentTok{# kalman filter equations}
        \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\NormalTok{n)\{}
            \CommentTok{# predict}
\NormalTok{            m_ <-}\StringTok{ }\NormalTok{stateTransition }\OperatorTok{%*%}\StringTok{ }\NormalTok{state[i}\DecValTok{-1}\NormalTok{, ]}
\NormalTok{            S_ <-}\StringTok{ }\NormalTok{stateTransition }\OperatorTok{%*%}\StringTok{ }\NormalTok{S }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(stateTransition) }\OperatorTok{+}\StringTok{ }\NormalTok{processNoise}

            \CommentTok{# update}
\NormalTok{            S <-}\StringTok{ }\NormalTok{outputMatrix }\OperatorTok{%*%}\StringTok{ }\NormalTok{S_ }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(outputMatrix) }\OperatorTok{+}\StringTok{ }\NormalTok{measurementNoise}
\NormalTok{            K <-}\StringTok{ }\NormalTok{S_ }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(outputMatrix) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{solve}\NormalTok{(S)}
\NormalTok{            m <-}\StringTok{ }\NormalTok{m_ }\OperatorTok{+}\StringTok{ }\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{(measurements[i,] }\OperatorTok{-}\StringTok{ }\NormalTok{outputMatrix }\OperatorTok{%*%}\StringTok{ }\NormalTok{m_)}
\NormalTok{            S <-}\StringTok{ }\NormalTok{S_ }\OperatorTok{-}\StringTok{ }\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{S }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(K)}

            \CommentTok{# save}
\NormalTok{            state[i,] <-}\StringTok{ }\NormalTok{m}
\NormalTok{        \}}
\NormalTok{    \}}
    \CommentTok{# return state  }
    \KeywordTok{return}\NormalTok{(state)}
\NormalTok{\}}
\CommentTok{#####}

\CommentTok{#####}
\CommentTok{# functions usind in deriving the recursive non linear regression extended kalman-filter solution}
\CommentTok{# exponential model with one variable}
\NormalTok{h <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, beta) \{ }\KeywordTok{exp}\NormalTok{(x}\OperatorTok{*}\NormalTok{beta) \}}

\CommentTok{# derivative of the exponential function w.r.t. the state}
\NormalTok{H <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, beta) \{ }\KeywordTok{h}\NormalTok{(x, beta)}\OperatorTok{/}\NormalTok{x \}}
\CommentTok{#####}

\CommentTok{#####}
\CommentTok{# functions for the importance sampling example}
\CommentTok{# function of the random variable}
\NormalTok{f <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}\KeywordTok{cos}\NormalTok{(x)}\OperatorTok{^}\DecValTok{2}\NormalTok{ \}}

\CommentTok{# normal density}
\NormalTok{d_norm  <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, }\DataTypeTok{mu=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sig=}\DecValTok{1}\NormalTok{) \{}\DecValTok{1}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{pi)}\OperatorTok{*}\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\StringTok{ }\NormalTok{(x}\OperatorTok{-}\NormalTok{mu)}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{sig}\OperatorTok{^}\DecValTok{2}\NormalTok{))\}}

\CommentTok{# laplace distribution density}
\NormalTok{dlaplace <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, }\DataTypeTok{b=}\DecValTok{1}\NormalTok{, }\DataTypeTok{mu =}\DecValTok{0}\NormalTok{)\{ }\DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{b)}\OperatorTok{*}\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\KeywordTok{abs}\NormalTok{(x}\OperatorTok{-}\NormalTok{mu)}\OperatorTok{/}\NormalTok{b)\}}

\CommentTok{# generate random variables according to the laplace distribution}
\NormalTok{rlaplace <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(b, u)\{}
    \OperatorTok{-}\NormalTok{b}\OperatorTok{*}\KeywordTok{sign}\NormalTok{(u}\FloatTok{-0.5}\NormalTok{)}\OperatorTok{*}\KeywordTok{log}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\DecValTok{2}\OperatorTok{*}\KeywordTok{abs}\NormalTok{(u}\FloatTok{-0.5}\NormalTok{))}
\NormalTok{\}}
\CommentTok{#####}

\CommentTok{#####}
\CommentTok{# state transition function for the particle filter simulation}
\NormalTok{state_update <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x,t) \{  }\FloatTok{0.5}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\StringTok{ }\DecValTok{25}\OperatorTok{*}\NormalTok{x}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{x}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }\DecValTok{8}\OperatorTok{*}\KeywordTok{cos}\NormalTok{(}\FloatTok{1.2}\OperatorTok{*}\NormalTok{(t}\DecValTok{-1}\NormalTok{)) \}}

\CommentTok{# measurement / process transition function for the particle filter simulation}
\NormalTok{measurement_update <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{ x}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\DecValTok{20}\NormalTok{ \}}
\CommentTok{#####}

\CommentTok{#####}
\CommentTok{# state transition function for the particle filter simulation}
\NormalTok{stochastic_volatility_state  <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, a, b) \{ a }\OperatorTok{+}\StringTok{ }\NormalTok{b }\OperatorTok{*}\StringTok{ }\NormalTok{x \}}

\CommentTok{# measurement / process transition function for the particle filter simulation}
\NormalTok{stochastic_volatility_process  <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(state) \{ }\KeywordTok{exp}\NormalTok{(state) \}}
\CommentTok{#####}
\end{Highlighting}
\end{Shaded}

The following three figures present the results of the Kalman-Filter
estimation of three different stochastic systems. A thourough
statistical performance evaluation will be omitted at this point, and we
are going to limit the evaluation to the visual inspection of the
results. Figure 1 presents a constant model with gaussian noise, which
changes its mean over time. Figure 2 represents sine measurements with
gaussian noise, which is a non-linear stochastic process. Figure 3
depicts a random walk in two dimensions, where movement in either
direction is dependent on the previous position plus gaussian noise.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate n constant measurements whose mean changes after n/2 measurements}
\CommentTok{# random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\CommentTok{# init n}
\NormalTok{n <-}\StringTok{ }\DecValTok{1000}

\CommentTok{# save results}
\NormalTok{time_var_const_mean <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n)}

\CommentTok{# generate time depended constant data with gaussian noise}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n)\{}
\NormalTok{        time_var_const_mean[i] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{5}\NormalTok{, }\DataTypeTok{sd =} \FloatTok{0.1}\NormalTok{)}
    \ControlFlowTok{if}\NormalTok{( i }\OperatorTok{<}\StringTok{ }\NormalTok{n}\OperatorTok{/}\DecValTok{2}\NormalTok{)\{}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{        time_var_const_mean[i] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{10}\NormalTok{, }\DataTypeTok{sd =} \FloatTok{0.1}\NormalTok{)}
\NormalTok{    \}}
\NormalTok{\}}

\CommentTok{# apply the kalman filter}
\NormalTok{kf_const_time_var_mean <-}\StringTok{ }\KeywordTok{kalman_filter}\NormalTok{( }\DataTypeTok{measurements =}\NormalTok{ time_var_const_mean, }\DataTypeTok{stateTransition =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{), }
    \DataTypeTok{processNoise =} \KeywordTok{diag}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)), }\DataTypeTok{outputMatrix =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{),  }\DataTypeTok{measurementNoise =} \KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{ncol=}\DecValTok{1}\NormalTok{)}
\NormalTok{)}

\KeywordTok{plot}\NormalTok{(time_var_const_mean, }\DataTypeTok{pch=}\DecValTok{19}\NormalTok{, }
    \DataTypeTok{main =} \StringTok{"Kalman-Filter Mean-Shift Model"}\NormalTok{,}
    \DataTypeTok{xlab =} \StringTok{"Time"}\NormalTok{,}
    \DataTypeTok{ylab =} \StringTok{"Measurements"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(kf_const_time_var_mean[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{col=}\StringTok{'red'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{Kalman_F_files/figure-latex/unnamed-chunk-2-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# second k-f example using sine function with additive gaussian noise }
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\CommentTok{# generate data}
\NormalTok{sine <-}\StringTok{ }\KeywordTok{sin}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\NormalTok{pi, pi, }\DataTypeTok{length.out=}\NormalTok{n)) }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n,}\DataTypeTok{sd=}\FloatTok{0.5}\NormalTok{)}

\CommentTok{# apply k-f}
\NormalTok{kf_sine <-}\StringTok{ }\KeywordTok{kalman_filter}\NormalTok{(}\DataTypeTok{measurements =}\NormalTok{ sine, }\DataTypeTok{stateTransition =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{), }\DataTypeTok{processNoise =} \KeywordTok{diag}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)), }
    \DataTypeTok{outputMatrix =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{), }\DataTypeTok{measurementNoise =} \KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{ncol=}\DecValTok{1}\NormalTok{)}
\NormalTok{)}

\KeywordTok{plot}\NormalTok{(sine, }\DataTypeTok{pch=}\DecValTok{19}\NormalTok{, }
    \DataTypeTok{main =} \StringTok{"Kalman-Filter Sine Function"}\NormalTok{,}
    \DataTypeTok{xlab =} \StringTok{"Time"}\NormalTok{,}
    \DataTypeTok{ylab =} \StringTok{"Measurements"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(kf_sine[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{col=}\StringTok{'red'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{Kalman_F_files/figure-latex/unnamed-chunk-3-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# generate two dimensional random walk, where the current position depends only on the previous one}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\CommentTok{# save data}
\NormalTok{random_walk <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{n)}

\CommentTok{# generte random walk}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\NormalTok{n)\{}
\NormalTok{    random_walk[i, }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{random_walk[i}\DecValTok{-1}\NormalTok{, }\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{sd=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{    random_walk[i, }\DecValTok{2}\NormalTok{] <-}\StringTok{ }\NormalTok{random_walk[i}\DecValTok{-1}\NormalTok{, }\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{sd=}\FloatTok{0.5}\NormalTok{)}
\NormalTok{\}}

\CommentTok{# apply k-f}
\NormalTok{kf_random_walk <-}\StringTok{ }\KeywordTok{kalman_filter}\NormalTok{(}\DataTypeTok{measurements =}\NormalTok{ random_walk, }\DataTypeTok{stateTransition =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{4}\NormalTok{), }
    \DataTypeTok{processNoise =} \KeywordTok{diag}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{)), }\DataTypeTok{outputMatrix =} \KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ncol =} \DecValTok{4}\NormalTok{),  }\DataTypeTok{measurementNoise =} \KeywordTok{diag}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{)}

\CommentTok{# save grafics}
\KeywordTok{plot}\NormalTok{(random_walk, }\DataTypeTok{type=}\StringTok{'l'}\NormalTok{,, }
    \DataTypeTok{main =} \StringTok{"Kalman-Filter Random Walk"}\NormalTok{,}
    \DataTypeTok{xlab =} \StringTok{"X Direction"}\NormalTok{,}
    \DataTypeTok{ylab =} \StringTok{"Y Direction"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(kf_random_walk[,}\DecValTok{1}\NormalTok{], kf_random_walk[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{col=}\StringTok{'red'}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{Kalman_F_files/figure-latex/unnamed-chunk-4-1} \end{center}

\hypertarget{recursive-regression-models}{%
\subsection{Recursive Regression
Models}\label{recursive-regression-models}}

The Kalman-Filter can also be used to recursively estimate the state of
a linear regression problem. The recursive solution has the same
posterior distribution as the batch solution, but it allows to monitor
the evolution of the state. In the following we present a simulation to
clarify this point. Using the following model to generate a dependent
variable \(y_t\) based on the true state \(\beta = (2,6)^T\) and
measurements \(x_t\):

\[
\begin{align*} 
    y_t &= \beta_{1,t} + \beta_{2,t}\cdot x_t + \varepsilon_t, \ \varepsilon_t \sim \mathcal{N}(0,1)
\end{align*}
\]

and the following state-space representation of the model:

\[
\begin{align*}
    \beta_t &= \beta_{t-1} \\
    y_t &= (1, x_t)^T \cdot \beta_t + \varepsilon_t, \ \varepsilon_t \sim \mathcal{N}(0,1)
\end{align*}
\]

The following code generates the respective models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# recursive linear regression}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\CommentTok{# explantatory variables with intercept}
\NormalTok{X <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n), }\KeywordTok{rnorm}\NormalTok{(n))}

\CommentTok{# true state}
\NormalTok{beta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{)}

\CommentTok{# gaussian model}
\NormalTok{y_lin <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{mean=}\NormalTok{ X}\OperatorTok{%*%}\NormalTok{beta, }\DecValTok{1}\NormalTok{)}

\CommentTok{# save state}
\NormalTok{beta_lin_est  <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{ncol=}\NormalTok{n, }\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{)}

\CommentTok{# state covariance}
\NormalTok{Sig <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}

\CommentTok{# not using the kalman filter function because for recursive regression}
\CommentTok{# the predict step is not required, thus there is no state transition in this model}
\CommentTok{# run the equations}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\NormalTok{n)\{}
\NormalTok{    S  <-}\StringTok{ }\NormalTok{X[i,,drop=}\OtherTok{FALSE}\NormalTok{] }\OperatorTok{%*%}\StringTok{ }\NormalTok{Sig }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(X[i,,}\DataTypeTok{drop=}\OtherTok{FALSE}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{    K  <-}\StringTok{ }\NormalTok{Sig }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(X[i,,}\DataTypeTok{drop=}\OtherTok{FALSE}\NormalTok{]) }\OperatorTok{/}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(S)}
\NormalTok{    beta_lin_est[,i]  <-}\StringTok{ }\NormalTok{beta_lin_est[,i}\DecValTok{-1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{(y_lin[i] }\OperatorTok{-}\StringTok{ }\NormalTok{X[i, ] }\OperatorTok{%*%}\StringTok{ }\NormalTok{beta_lin_est[,i}\DecValTok{-1}\NormalTok{])    }
\NormalTok{    Sig <-}\StringTok{ }\NormalTok{Sig }\OperatorTok{-}\StringTok{ }\NormalTok{K}\OperatorTok{%*%}\NormalTok{S}\OperatorTok{%*%}\KeywordTok{t}\NormalTok{(K)}
\NormalTok{\}}

\CommentTok{# save plot}
\KeywordTok{plot}\NormalTok{(X[,}\DecValTok{2}\NormalTok{], y_lin,}
    \DataTypeTok{main =} \StringTok{"Measurement-Space"}\NormalTok{,}
    \DataTypeTok{xlab =} \StringTok{"X"}\NormalTok{,}
    \DataTypeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\KeywordTok{lm}\NormalTok{(y_lin }\OperatorTok{~}\StringTok{ }\NormalTok{X }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{ )}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{1}\NormalTok{], }\KeywordTok{lm}\NormalTok{(y_lin }\OperatorTok{~}\StringTok{ }\NormalTok{X }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{ )}\OperatorTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{], }\DataTypeTok{lw=}\DecValTok{5}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(beta_lin_est[}\DecValTok{1}\NormalTok{, n], beta_lin_est[}\DecValTok{2}\NormalTok{,n], }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{Kalman_F_files/figure-latex/unnamed-chunk-5-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# state space}
\KeywordTok{plot}\NormalTok{(beta_lin_est[}\DecValTok{1}\NormalTok{,}\OperatorTok{-}\DecValTok{1}\NormalTok{], beta_lin_est[}\DecValTok{2}\NormalTok{,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }
    \DataTypeTok{main =} \StringTok{"State-Space Evolution"}\NormalTok{,}
    \DataTypeTok{xlab =} \StringTok{"Intercept"}\NormalTok{,}
    \DataTypeTok{ylab =} \StringTok{"x"}\NormalTok{,}
    \DataTypeTok{type =} \StringTok{"l"}
\NormalTok{)}
\KeywordTok{points}\NormalTok{(beta_lin_est[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], beta_lin_est[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{], }\DataTypeTok{pch=}\DecValTok{19}\NormalTok{)}
\KeywordTok{text}\NormalTok{(beta_lin_est[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{], beta_lin_est[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{], }\StringTok{"First"}\NormalTok{, }\DataTypeTok{adj =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\FloatTok{0.3}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\KeywordTok{points}\NormalTok{(beta_lin_est[}\DecValTok{1}\NormalTok{, n], beta_lin_est[}\DecValTok{2}\NormalTok{,n], }\DataTypeTok{pch=}\DecValTok{19}\NormalTok{)}
\KeywordTok{text}\NormalTok{(beta_lin_est[}\DecValTok{1}\NormalTok{, n], beta_lin_est[}\DecValTok{2}\NormalTok{,n], }\StringTok{"Last"}\NormalTok{, }\DataTypeTok{adj =} \KeywordTok{c}\NormalTok{(}\FloatTok{1.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{))}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{v =} \DecValTok{2}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{lw =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \DecValTok{6}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{, }\DataTypeTok{lw =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{Kalman_F_files/figure-latex/unnamed-chunk-5-2} \end{center}

Figure 4 depicts the batch solution as a thick black-, and the recursive
solution as a red line. As we can see, the solutions are practically
identical. Figure 5 depicts the evolution of the state as additional
information becomes available. The red dotted lines represent the true
state. As we can see, the recursive solution moves continuously closer
to the true state.

Concluding, the Kalman-Filter is able to estimate linear- as well as
non-linear time dependent stochastic processes in multiple dimensions.

\hypertarget{extended-kalman-filter}{%
\section{Extended-Kalman-Filter}\label{extended-kalman-filter}}

The Kalman-Filter is the solution to a linear dynamic stochastic system.
If the restriction of linearity is dropped, but all other assumptions
maintained, the Bayesian filtering problem can be formulated as:

\[
\begin{equation*} 
    \begin{aligned}
        \textbf{x}_t &= \textbf{f}(\textbf{x}_{t-1}) + \textbf{q}_{t-1},\ \textbf{q}_{t-1} \sim \mathcal{N}(\textbf{0},\textbf{Q}_{t-1}) \\
        \textbf{y}_t &= \textbf{g}(\textbf{x}_t) + \textbf{r}_t, \ \textbf{r}_t \sim \mathcal{N}(\textbf{0},\textbf{R}_{t})          
    \end{aligned}
\end{equation*}
\]

The idea behind the Extended-Kalman-Filter is to calculate an
approximate solution, rather than an exact one. The main tool to achieve
the approximation is the Taylor-Series expansion of \(\textbf{f}\) and
\(\textbf{g}\). As with the derivation of the Kalman-Filter equations,
we begin with the prediction step.

\hypertarget{prediction}{%
\subsection{Prediction}\label{prediction}}

According to Taylor's theorem the first order approximation is:

\[
\begin{equation*}
    \begin{aligned}
        \textbf{f}(\textbf{x}_{t-1}) &\approx \textbf{f}(\textbf{m}_{t-1}) + \textbf{F}_x(\textbf{m}_{t-1})(\textbf{x}_{t-1} - \textbf{m}_{t-1})  \\
    \end{aligned}
\end{equation*}
\]

\[
\begin{equation*} 
    \begin{aligned}
        \nu_{t-1} &:= \textbf{f}(\textbf{m}_{t-1}) + \textbf{F}_x(\textbf{m}_{t-1})(\textbf{x}_{t-1} - \textbf{m}_{t-1})\\
    \end{aligned}
\end{equation*}    
\]

where \(\textbf{F}_x\) refers to the derivative of \(\textbf{f}\) with
respect to \(\textbf{x}_{t-1}\) and \(\textbf{m}_{t-1}\) is assumed to
be a known quantity. If we further assume that
\((\textbf{x}_{t-1} - \textbf{m}_{t-1}) \sim \mathcal{N}(\textbf{0}, \textbf{C}_{t-1})\),
then using Corollary 4.1 and the model assumptions:

\[
\begin{equation*}
    \nu_{t-1} \sim \mathcal{N}(\textbf{f}(\textbf{m}_{t-1}), \textbf{F}_x\textbf{C}_{t-1}\textbf{F}_x^T)
\end{equation*}
\]

and since:

\[
\begin{equation*} 
    \begin{aligned}
        \nu_{t-1} + \textbf{q}_{t-1} &= \begin{pmatrix}
            1,1
        \end{pmatrix}^T\begin{pmatrix}
            \nu_{t-1} \\
            \textbf{q}_{t-1}
        \end{pmatrix} \text{, thus: } \\
        \nu_{t-1} + \textbf{q}_{t-1} &\sim \mathcal{N}\begin{pmatrix}
                \begin{pmatrix}
                    1,1
                \end{pmatrix}^T \begin{pmatrix}
                    \textbf{f}(\textbf{m}_{t-1}) \\
                    \textbf{0}
                \end{pmatrix}, \begin{pmatrix}
                    \begin{pmatrix}
                        1,1
                    \end{pmatrix}^T \begin{pmatrix}
                        \textbf{F}_x\textbf{C}_{t-1}\textbf{F}_x^T & \textbf{0}\\
                        \textbf{0} & \textbf{Q}_{t-1}\\
                    \end{pmatrix} \begin{pmatrix}
                        1,1
                    \end{pmatrix}
                \end{pmatrix}
        \end{pmatrix} \\
    \end{aligned}
\end{equation*}
\]

\[
\begin{equation*}
    \begin{aligned}
        \textbf{m}_t & := \textbf{f}(\textbf{m}_{t-1}) \\
        \textbf{C}_t & := \textbf{F}_x\textbf{C}_{t-1}\textbf{F}_x^T + \textbf{Q}_{t-1}\\        
    \end{aligned}
\end{equation*}
\]

and obtain: \[
\begin{equation*}
    \nu_{t-1} + \textbf{q}_{t-1} \sim \mathcal{N}(\textbf{m}_t, \textbf{C}_t)\\
\end{equation*}
\]

Since
\(\textbf{x}_t| \textbf{y}_{1:t-1}\simeq \nu_{t-1} + \textbf{q}_{t-1}\):
\[
\begin{equation*} 
    \begin{aligned}
        \textbf{x}_t|\textbf{y}_{1:t-1} &\simeq \mathcal{N}(\textbf{m}_t, \textbf{C}_t)\\
        \textbf{x}_t - \textbf{m}_{t}|\textbf{y}_{1:t-1} &\simeq \mathcal{N}(\textbf{0}, \textbf{C}_t)\\
    \end{aligned}
\end{equation*}
\]

The distribution of \(\textbf{x}_t - \textbf{m}_{t}|\textbf{y}_{1:t-1}\)
is derived by setting \(A = I\) and \(b = -\textbf{m}_{t}\).

\hypertarget{update-1}{%
\subsection{Update}\label{update-1}}

The derivation for the update equations follows the same arguments as in
the prediction step, by using the information obtained in the prediction
step:

\[
\begin{equation*}
    \begin{aligned}
        \textbf{g}(\textbf{x}_{t}) &\approx \textbf{g}(\textbf{m}_{t}) + \textbf{G}_x(\textbf{m}_{t})(\textbf{x}_{t} - \textbf{m}_{t})  \\
    \end{aligned}
\end{equation*}
\]

\[
\begin{equation*} 
    \begin{aligned}
        \gamma_t := \textbf{g}(\textbf{m}_{t}) + \textbf{G}_x(\textbf{m}_{t})(\textbf{x}_{t} - \textbf{m}_{t})
    \end{aligned}
\end{equation*}    
\]

Using same arguments as before, we obtain:

\[
\begin{equation*}
    \textbf{y}_t| \textbf{y}_{1:t-1}\simeq \gamma_t + r_t \sim \mathcal{N}(\textbf{g}(\textbf{m}_{t}), \textbf{G}_x\textbf{C}_t\textbf{G}_x^T + \textbf{R}_t)
\end{equation*}
\]

and thus the joint distribution
\(p(\textbf{x}_t, \textbf{y}_t| \textbf{y}_{1:t-1})\):

\[
\begin{equation*}
    \begin{pmatrix}
        \textbf{x}_t \\
        \textbf{y}_t
    \end{pmatrix} \sim \mathcal{N} \begin{pmatrix}
        \begin{pmatrix}
            \textbf{m}_t\\
            \textbf{g}(\textbf{m}_{t})\\
        \end{pmatrix}, \begin{pmatrix}
            \textbf{C}_t & \textbf{C}_t \textbf{G}_x^T \\
            \textbf{G}_x\textbf{C}_t & \textbf{G}_x\textbf{C}_t\textbf{G}_x^T + \textbf{R}_t
        \end{pmatrix}
    \end{pmatrix}
\end{equation*}
\]

from which we can calculate the distribution of
\(\textbf{x}_t|\textbf{y}_{1:t}\):

\[
    \begin{equation*}
        \begin{aligned}
            \mu_t &:= \textbf{m}_t + \textbf{C}_t \textbf{G}_x^T(\textbf{G}_x\textbf{C}_t\textbf{G}_x^T + \textbf{R}_t)^{-1}(\textbf{y}_t - \textbf{g}(\textbf{m}_{t})) \\
            \Sigma_t &:=  \textbf{C}_t - \textbf{C}_t \textbf{G}_x^T(\textbf{G}_x\textbf{C}_t\textbf{G}_x^T + \textbf{R}_t)^{-1}\textbf{G}_x\textbf{C}_t\\
        \end{aligned}
    \end{equation*}
\] and thus we obtain:

\[
\begin{equation*}
    \textbf{x}_t|\textbf{y}_{1:t}\sim \mathcal{N}(\mu_t, \Sigma_t)
\end{equation*}
\] Using the first order Taylor series expansion, we obtained a closed
form solution to a non-linear dynamic stochastic system.

\hypertarget{algorithm-1}{%
\subsection{Algorithm}\label{algorithm-1}}

To summarize the algorithm:

\begin{itemize}
\tightlist
\item
  predict:

  \begin{itemize}
  \tightlist
  \item
    \(\textbf{m}_t = \textbf{f}(\textbf{m}_{t-1})\)
  \item
    \(\textbf{C}_t = \textbf{F}_x\textbf{C}_{t-1}\textbf{F}_x^T + \textbf{Q}_{t-1}\)
  \end{itemize}
\item
  update:

  \begin{itemize}
  \tightlist
  \item
    \(\nu_t = \textbf{y}_t - \textbf{g}(\textbf{m}_{t})\)
  \item
    \(\textbf{S}_t = \textbf{G}_x\textbf{C}_t\textbf{G}_x^T + \textbf{R}_t\)
  \item
    \(\textbf{K}_t = \textbf{C}_t \textbf{G}_x^T\textbf{S}_t^{-1}\)
  \item
    \(\mu_t = \textbf{m}_t + \textbf{K}_t\nu_t\)
  \item
    \(\Sigma_t = \textbf{C}_t-\textbf{K}_t\textbf{S}_t\textbf{K}_t^T\)
  \end{itemize}
\end{itemize}

\hypertarget{simulation}{%
\subsection{Simulation}\label{simulation}}

The Extended-Kalman-Filter can be used to estimate non-linear regression
problems. In this simulation we generated 1000 artificial observations
using:

\[
\begin{align*}
    y_t &= exp(\beta \cdot x_t) + \varepsilon_t, \ \varepsilon_t \sim \mathcal{N}(0,1)
\end{align*}
\] which is a non-linear function of the state, and the following
state-space representation of the model:

\[
\begin{align*}
    \beta_t &= \beta_{t-1} \\
    y_t &= exp(\beta_t \cdot x_t) + \varepsilon_t, \ \varepsilon_t \sim \mathcal{N}(0,1)
\end{align*}
\]

Figure 6 presents the generated data in black and the result of the
Extended-Kalman-Filter as the red line. As we can see, the
Extended-Kalman-Filter captures the \(x-y\) relationship for
\(x \leq 1\) quite well, and for \(x > 1\) appears to consistently
underestimate the measurements. Figure 7 shows that the state indeed
levels out constantly below the true state. This divergence might be due
to a misspecified model, which we know is not the case, or due to the
fact that large x-values are unlikely to occur, and there is no evidence
to support a proper estimation of these values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{##################################################################################################################}
\CommentTok{############################# EXTENDED KALMAN FILTER SIMULATION  #################################################}
\CommentTok{##################################################################################################################}

\CommentTok{# recursive non linear regression}
\CommentTok{# additional observations}
\CommentTok{# first try}
\NormalTok{n <-}\StringTok{ }\DecValTok{1000}

\CommentTok{# state of the model}
\NormalTok{beta_non_lin_est  <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n}\OperatorTok{+}\DecValTok{1}\NormalTok{)}

\CommentTok{# generate covariable, just standard normal observations }
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{x  <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n))}

\CommentTok{# generate gaussian measurements}
\NormalTok{y_non_lin <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{mean=} \KeywordTok{h}\NormalTok{(x, }\FloatTok{0.9}\NormalTok{), }\DecValTok{1}\NormalTok{)}

\CommentTok{# state variance }
\NormalTok{Sig <-}\StringTok{ }\DecValTok{1}

\CommentTok{# run the calculation for the extended kalman filter according to formula}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n)\{}
\NormalTok{    v  <-}\StringTok{ }\NormalTok{y_non_lin[i] }\OperatorTok{-}\StringTok{ }\KeywordTok{h}\NormalTok{(x[i], beta_non_lin_est[i])}
\NormalTok{    H <-}\StringTok{ }\KeywordTok{h}\NormalTok{(x[i], beta_non_lin_est[i])}\OperatorTok{/}\NormalTok{x[i]}
\NormalTok{    S  <-}\StringTok{ }\NormalTok{H }\OperatorTok{*}\StringTok{ }\NormalTok{Sig }\OperatorTok{*}\StringTok{ }\NormalTok{H }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{    K  <-}\StringTok{ }\NormalTok{Sig }\OperatorTok{*}\StringTok{ }\NormalTok{H }\OperatorTok{/}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(S)}
\NormalTok{    beta_non_lin_est[}\DecValTok{1}\OperatorTok{+}\NormalTok{i]  <-}\StringTok{ }\NormalTok{beta_non_lin_est[i] }\OperatorTok{+}\StringTok{ }\NormalTok{K }\OperatorTok{*}\StringTok{ }\NormalTok{v}
\NormalTok{    Sig <-}\StringTok{ }\NormalTok{Sig }\OperatorTok{-}\StringTok{ }\NormalTok{K}\OperatorTok{*}\NormalTok{S}\OperatorTok{*}\KeywordTok{t}\NormalTok{(K)}
\NormalTok{\}}

\CommentTok{# save grafics}
\KeywordTok{plot}\NormalTok{(x, y_non_lin, }
    \DataTypeTok{main =} \StringTok{"Measurements"}\NormalTok{,}
    \DataTypeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \DataTypeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{h}\NormalTok{(x, beta_non_lin_est[}\OperatorTok{-}\NormalTok{n]), }\DataTypeTok{col=}\StringTok{'red'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{Kalman_F_files/figure-latex/unnamed-chunk-6-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(beta_non_lin_est, }\DataTypeTok{type=}\StringTok{'l'}\NormalTok{,}
 \DataTypeTok{main =} \StringTok{"State-Space"}\NormalTok{,}
 \DataTypeTok{xlab =} \StringTok{"Iteration"}\NormalTok{,}
 \DataTypeTok{ylab =} \StringTok{"State"}
\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \FloatTok{0.9}\NormalTok{, }\DataTypeTok{col =} \StringTok{'red'}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{Kalman_F_files/figure-latex/unnamed-chunk-6-2} \end{center}

The latter assumption can be tested by increasing the number of
observations generated to 10000:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# recursive non linear regression}
\CommentTok{# additional observations}
\CommentTok{# second try increase n}
\NormalTok{n <-}\StringTok{ }\DecValTok{10000}

\CommentTok{# state of the model}
\NormalTok{beta_non_lin_est  <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n}\OperatorTok{+}\DecValTok{1}\NormalTok{)}

\CommentTok{# generate covariable, just standard normal observations }
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{x  <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n))}

\CommentTok{# generate gaussian measurements}
\NormalTok{y_non_lin <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{mean=} \KeywordTok{h}\NormalTok{(x, }\FloatTok{0.9}\NormalTok{), }\DecValTok{1}\NormalTok{)}

\CommentTok{# state variance }
\NormalTok{Sig <-}\StringTok{ }\DecValTok{1}

\CommentTok{# run the calculation for the extended kalman filter according to formula}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n)\{}
\NormalTok{    v  <-}\StringTok{ }\NormalTok{y_non_lin[i] }\OperatorTok{-}\StringTok{ }\KeywordTok{h}\NormalTok{(x[i], beta_non_lin_est[i])}
\NormalTok{    H <-}\StringTok{ }\KeywordTok{h}\NormalTok{(x[i], beta_non_lin_est[i])}\OperatorTok{/}\NormalTok{x[i]}
\NormalTok{    S  <-}\StringTok{ }\NormalTok{H }\OperatorTok{*}\StringTok{ }\NormalTok{Sig }\OperatorTok{*}\StringTok{ }\NormalTok{H }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{    K  <-}\StringTok{ }\NormalTok{Sig }\OperatorTok{*}\StringTok{ }\NormalTok{H }\OperatorTok{/}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(S)}
\NormalTok{    beta_non_lin_est[}\DecValTok{1}\OperatorTok{+}\NormalTok{i]  <-}\StringTok{ }\NormalTok{beta_non_lin_est[i] }\OperatorTok{+}\StringTok{ }\NormalTok{K }\OperatorTok{*}\StringTok{ }\NormalTok{v}
\NormalTok{    Sig <-}\StringTok{ }\NormalTok{Sig }\OperatorTok{-}\StringTok{ }\NormalTok{K}\OperatorTok{*}\NormalTok{S}\OperatorTok{*}\KeywordTok{t}\NormalTok{(K)}
\NormalTok{\}}

\CommentTok{# save grafics}
\KeywordTok{plot}\NormalTok{(x, y_non_lin, }
    \DataTypeTok{main =} \StringTok{"Measurements"}\NormalTok{,}
    \DataTypeTok{xlab =} \StringTok{"x"}\NormalTok{,}
    \DataTypeTok{ylab =} \StringTok{"y"}
\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{h}\NormalTok{(x, beta_non_lin_est[}\OperatorTok{-}\NormalTok{n]), }\DataTypeTok{col=}\StringTok{'red'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Kalman_F_files/figure-latex/unnamed-chunk-7-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(beta_non_lin_est, }\DataTypeTok{type=}\StringTok{'l'}\NormalTok{,}
 \DataTypeTok{main =} \StringTok{"State-Space"}\NormalTok{,}
 \DataTypeTok{xlab =} \StringTok{"Iteration"}\NormalTok{,}
 \DataTypeTok{ylab =} \StringTok{"State"}
\NormalTok{)}
\KeywordTok{abline}\NormalTok{(}\DataTypeTok{h =} \FloatTok{0.9}\NormalTok{, }\DataTypeTok{col =} \StringTok{'red'}\NormalTok{, }\DataTypeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Kalman_F_files/figure-latex/unnamed-chunk-7-2} \end{center}

As we can see, increasing the number of observations leads to a better
estimation throughout the range of the x-values.

Concluding, the Kalman-Filter is a closed form solution for linear-, and
the Extended-Kalman-Filter an approximate solution for non-linear
dynamic stochastic systems, given that state and process noise are
gaussian. If the assumption of the gaussian distribution of either state
or process is dropped, the Kalman-Filter is not applicable anymore, and
a more general framework is required to handle non gaussian
distributions.

\hypertarget{particle-filter}{%
\section{Particle Filter}\label{particle-filter}}

Given the state-space model and providing an explicit form of the model
such as:

\[
\begin{align*}
    \textbf{x}_t &= \textbf{f}(\textbf{x}_{t-1}) + \varepsilon_{t-1} \\
    \textbf{y}_t &= \textbf{g}(\textbf{x}_{t}) + \nu_t \\
\end{align*}
\]

we might not be able to find an exact solution to either problem. Thus
instead of trying to find an exact solution, we are interested in
finding:

\[
\begin{equation*}
    \begin{aligned}
        \mathbb{E}[\textbf{y}_t] &= \mathbb{E}[\textbf{g}(\textbf{x}_{t})+ \nu_t] \\
        &= \mathbb{E}[\textbf{g}(\textbf{x}_{t})] \\
        &= \int  \textbf{g}(\textbf{x}_{t}) p(\textbf{x}_{t}|\textbf{y}_{1:t}) \textbf{dx}_t
    \end{aligned}
\end{equation*}
\]

assuming \(\mathbb{E}[\nu_t] = 0\). If we can't evaluate this integral
analytically or numerically, we can apply monte carlo integration due to
the fact that:

\[
\begin{equation*} 
    \mathbb{E}[\textbf{g}(\textbf{x}_{t})] \approx \frac{1}{N} \sum_{i=1}^N \textbf{g}(\textbf{x}_{t}^{(i)})
\end{equation*}
\]

where \(\textbf{x}_{t}^{(i)},\ i = 1,...,N\) are samples from
\(p(\textbf{x}_{t}|\textbf{y}_{1:t})\). If we are unable to sample from
\(p(\textbf{x}_{t}|\textbf{y}_{1:t})\), we can use importance sampling
to evaluate \(\mathbb{E}[\textbf{g}(\textbf{x}_{t})]\).

\hypertarget{importance-sampling}{%
\subsection{Importance Sampling}\label{importance-sampling}}

The key idea behind importance sampling is to replace the sample of a
random variable \(\textbf{x}^{(i)} \sim G(\cdot)\), where \(G(\cdot)\)
is the target distribution, with sampling from an importance
distribution \(\textbf{x}^{(i)} \sim \pi(\cdot)\). The algorithm
consists of the following steps:

\begin{itemize}
\tightlist
\item
  Draw samples from: \(\textbf{x}^{(i)} \sim \pi(\cdot)\)
\item
  Calculate weights:
  \(\textbf{w}^{*(i)} = \frac{g(\textbf{x}^{(i)})}{\pi(\textbf{x}^{(i)})}\)
\item
  Normalize weights:
  \(\textbf{w}^{(i)} = \frac{\textbf{w}^{*(i)}}{\sum \textbf{w}^{*(i)}}\)
\item
  Calculate quantity of interest.
\end{itemize}

Applying this algorithm to calculate
\(\mathbb{E}[\textbf{g(x)}|\textbf{y}_{1:t}]\), we obtain the following
results for calculating the weights: \[
\begin{equation}
    \begin{aligned}
        \mathbb{E}[\textbf{g(x)}|\textbf{y}_{1:t}] &= \int  \textbf{g}(\textbf{x}) p(\textbf{x}|\textbf{y}_{1:t}) \textbf{dx} \\
                                 &= \int  \textbf{g}(\textbf{x}) \frac{p(\textbf{y}_{1:t}|\textbf{x}) p(\textbf{x}) }{\int p(\textbf{y}_{1:t}|\textbf{x})p(\textbf{x})} \textbf{dx} \\
                                 &=    \frac{ \int\textbf{g}(\textbf{x}) (\frac{p(\textbf{y}_{1:t}|\textbf{x}) p(\textbf{x}) }{\pi(\textbf{x})}) \pi(\textbf{x}) }
                                 {\int \frac{p(\textbf{y}_{1:t}|\textbf{x}) p(\textbf{x}) }{\pi(\textbf{x})} \pi(\textbf{x}) \textbf{dx}} \textbf{dx} \\
                                 &\approx \frac{ \frac{1}{N} \sum_{i=1}^N \textbf{g}(\textbf{x}^{(i)}) (\frac{p(\textbf{y}_{1:t}|\textbf{x}^{(i)}) p(\textbf{x}^{(i)}) }{\pi(\textbf{x}^{(i)}|\textbf{y}_{1:t})})  }
                                 {\frac{1}{N} \sum_{j=1}^N \frac{p(\textbf{y}_{1:t}|\textbf{x}^{(j)}) p(\textbf{x}^{(j)}) }{\pi(\textbf{x}^{(j)}|\textbf{y}_{1:t})} } \\
                                 &= \sum_{i=1}^N \bigg(\frac{  \frac{p(\textbf{y}_{1:t}|\textbf{x}^{(i)}) p(\textbf{x}^{(i)}) }{\pi(\textbf{x}^{(i)}|\textbf{y}_{1:t})}  }
                                 {\sum_{j=1}^N \frac{p(\textbf{y}_{1:t}|\textbf{x}_t^{(j)}) p(\textbf{x}_t^{(j)}) }{\pi(\textbf{x}^{(j)}|\textbf{y}_{1:t})} } \bigg)\textbf{g}(\textbf{x}^{(i)}) \\
    \end{aligned}
\end{equation}
\]

\[
\begin{equation*}
    w^{(i)} := \frac{  \frac{p(\textbf{y}_{1:t}|\textbf{x}^{(i)}) p(\textbf{x}^{(i)}) }{\pi(\textbf{x}_{t}^{(i)})}  }
    {\sum_{j=1}^N \frac{p(\textbf{y}_{1:t}|\textbf{x}^{(j)}) p(\textbf{x}^{(j)}) }{\pi(\textbf{x}^{(j)})} }
\end{equation*}
\]

Then: \[
\begin{equation*}
    \mathbb{E}[\textbf{g(x)}|\textbf{y}_{1:t}] = \sum_{i=1}^N w^{(i)}\textbf{g}(\textbf{x}^{(i)})
\end{equation*}
\]

and the posterior density can be approximated according to: \[
\begin{equation*}
    p(\textbf{x}|\textbf{y}_{1:t}) \approx \sum_{i=1}^N w^{(i)}\delta(\textbf{x} - \textbf{x}^{(i)})
\end{equation*}
\]

\hypertarget{example}{%
\subsection{Example}\label{example}}

Due to its critical role in the Particle-Filter, this section presents
an example for the importance sampling algorithm. Let
\(X \sim \mathcal{N}(0,1)\) and assume further we are interested in
calculating \(\mathbb{E}[cos(X)^2]\). The first step in the importance
sampling algorithm consists of choosing the importance density
\(\pi(x)\). To choose the importance density, we are interested in a
density which is as ``close'' as possible to the quantity of interest,
since theoretically all distributions for which
\(\mathcal{S}_X \subseteq \mathcal{S}_{\pi}\) are applicable as
importance distributions.

The following figure presents different densities in the interval
\(x \in [-3,3]\). The black line depicts the function
\(cos(x)^2*dnorm(x)\), the green line is the density of a standard
normal-, the red line the density of a Laplace \((\mu = 0, b=1)\) - and
the blue line the density of a uniform \((-3,3)\) distributed random
variable. The figure shows that the target density, the standard normal
in this case, is not obviously the best density to choose from. The
Laplace density is in certain areas closer to \(cos(x)^2*dnorm(x)\) than
the standard normal density. Using numeric integration we obtain:
\(\int_{-3}^3cos(x)^2*dnorm(x)dx \approx 0.5651\). Next we draw \(1000\)
samples from each distribution and calculate:

\[
\begin{equation*}
    \frac{1}{N} \frac{\sum_{i=1}^N cos(x^{(i)})^2*dnorm(x^{(i)})}{\pi(x^{(i)})} 
\end{equation*}
\]

for which we obtained \(0.56\) for the Laplace-, \(0.56\) for the
standard normal- and \(0.55\) for the uniform distribution. Hence they
are all close to the numeric solution, but the Laplace distribution is
the closest. Thus we would choose the Laplace distribution as the
importance distribution.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{##################################################################################################################}
\CommentTok{#############################     PARTICLE FILTER SIMULATION     #################################################}
\CommentTok{##################################################################################################################}

\CommentTok{# importance sampling example}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\CommentTok{# generate x-values in the range of -3,3}
\NormalTok{x <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DataTypeTok{by=}\FloatTok{0.1}\NormalTok{)}

\CommentTok{# plot importance densities and target function}
\KeywordTok{plot}\NormalTok{(x, }\KeywordTok{f}\NormalTok{(x)}\OperatorTok{*}\KeywordTok{dnorm}\NormalTok{(x), }\DataTypeTok{type=}\StringTok{'l'}\NormalTok{, }\DataTypeTok{xlim=}\KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{), }\DataTypeTok{ylim=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{), }
    \DataTypeTok{main=}\StringTok{"Comparing Distributions"}\NormalTok{,}
    \DataTypeTok{ylab =} \StringTok{"cos(x^2) * dnorm"}
\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{dlaplace}\NormalTok{(x),}\DataTypeTok{col=}\StringTok{'red'}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{dnorm}\NormalTok{(x),}\DataTypeTok{col=}\StringTok{'green'}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x, }\KeywordTok{dunif}\NormalTok{(x, }\DecValTok{-3}\NormalTok{,}\DecValTok{3}\NormalTok{),}\DataTypeTok{col=}\StringTok{'blue'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{Kalman_F_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# perform monte carlo integration to calculate the expected values}
\NormalTok{n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(x)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{norm_samples  <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\NormalTok{lapl_samples <-}\StringTok{ }\KeywordTok{rlaplace}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{runif}\NormalTok{(n))}
\NormalTok{unif_samples_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(n, }\DecValTok{-3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\KeywordTok{mean}\NormalTok{( }\KeywordTok{f}\NormalTok{(lapl_samples) }\OperatorTok{*}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(lapl_samples) }\OperatorTok{/}\StringTok{ }\KeywordTok{dlaplace}\NormalTok{(lapl_samples, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6355556
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{( }\KeywordTok{f}\NormalTok{(unif_samples_}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(unif_samples_}\DecValTok{2}\NormalTok{) }\OperatorTok{/}\StringTok{ }\KeywordTok{dunif}\NormalTok{(unif_samples_}\DecValTok{2}\NormalTok{, }\DecValTok{-3}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5900178
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{( }\KeywordTok{f}\NormalTok{(norm_samples) }\OperatorTok{*}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(norm_samples) }\OperatorTok{/}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(norm_samples))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5835453
\end{verbatim}

\hypertarget{sequential-importance-sampling}{%
\subsection{Sequential Importance
Sampling}\label{sequential-importance-sampling}}

To solve the estimation problem: \[
\begin{equation*}
    \begin{aligned}
        p(\textbf{x}_{0:t}|\textbf{y}_{1:t}) &\propto p(\textbf{y}_{t}|\textbf{x}_{0:t},\textbf{y}_{1:t-1}) \\
                                             &= p(\textbf{y}_{t}|\textbf{x}_{t})p(\textbf{x}_{t}|\textbf{x}_{0:t-1}, \textbf{y}_{1:t-1}) \\
                                             &= p(\textbf{y}_{t}|\textbf{x}_{t})p(\textbf{x}_{t}|\textbf{x}_{t-1}) p(\textbf{x}_{0:t-1}|\textbf{y}_{1:t-1}) \\
    \end{aligned}
\end{equation*}
\]

where the quantity \(p(\textbf{x}_{0:t}|\textbf{y}_{1:t})\) was
decomposed into 2 objects for which there is a model
\(p(\textbf{y}_{t}|\textbf{x}_{t})p(\textbf{x}_{t}|\textbf{x}_{t-1})\)
and one which is assumed to be known
\(p(\textbf{x}_{0:t-1}|\textbf{y}_{1:t-1})\). Since these quantities
might be too difficult to calculate analytically, we can apply the
importance sample algorithm sequentially and draw samples
\(\textbf{x}_t^{(i)} \sim \pi(\textbf{x}_{0:t}|\textbf{y}_{1:t})\) \[
\begin{equation*}
    \pi(\textbf{x}_{0:t}|\textbf{y}_{1:t}) = \pi(\textbf{x}_{t}|\textbf{x}_{0:t-1},\textbf{y}_{1:t})\pi(\textbf{x}_{0:t-1}|\textbf{y}_{1:t})
\end{equation*}
\] and we obtain: \[
\begin{equation*}
    \begin{aligned}
        w_t^{(i)} &\propto \frac{p(\textbf{y}_{t}|\textbf{x}_{t}^{(i)})p(\textbf{x}_{t}^{(i)}|\textbf{x}_{t-1}^{(i)}) }{\pi(\textbf{x}_{t}^{(i)}|\textbf{x}_{0:t-1}^{(i)},\textbf{y}_{1:t})} \frac{p(\textbf{x}_{0:t-1}^{(i)}|\textbf{y}_{1:t-1})}{\pi(\textbf{x}_{0:t-1}^{(i)}|\textbf{y}_{1:t})} \\
        w_{t-1}^{(i)} &\propto \frac{p(\textbf{x}_{0:t-1}^{(i)}|\textbf{y}_{1:t-1})}{\pi(\textbf{x}_{0:t-1}^{(i)}|\textbf{y}_{1:t})} \\
        w_t^{(i)} &\propto  w_{t-1}^{(i)} \frac{p(\textbf{y}_{t}|\textbf{x}_{t}^{(i)})p(\textbf{x}_{t}^{(i)}|\textbf{x}_{t-1}^{(i)}) }{\pi(\textbf{x}_{t}^{(i)}|\textbf{x}_{0:t-1}^{(i)},\textbf{y}_{1:t})}
    \end{aligned}
\end{equation*}
\]

Note that if we choose
\$\pi(\textbf{x}\_\{t\}\textsuperscript{\{(i)\}\textbar{}\textbf{x}\emph{\{0:t-1\}\textsuperscript{\{(i)\},\textbf{y}\emph{\{1:t\})
= p(\textbf{x}}\{t\}}\{(i)\}\textbar{}\textbf{x}}\{t-1\}}\{(i)\}) \$
then the problem simplifies to:

\[
\begin{equation*}
        w_t^{(i)} \propto  w_{t-1}^{(i)} p(\textbf{y}_{t}|\textbf{x}_{t}^{(i)})
\end{equation*}
\]

\hypertarget{sequential-importance-resampling}{%
\subsection{Sequential Importance
Resampling}\label{sequential-importance-resampling}}

Applying the sequential importance sampling leads to a phenomenon called
the degeneracy problem. The degeneracy problem refers to the phenomenon
that when applying the importance sampling algorithm sequentially, it
leads to all but one weight to be close to \(0\). To overcome this
problem, adding a resampling procedure, where the samples form the
posterior \(\pi(\textbf{x}_{0:t}|\textbf{y}_{1:t})\) are drawn form the
previous samples \(\textbf{x}_t^{(i)}\) with the probabilities given by
\(w_t^{(i)}\). These samples are also referred to as particles
\citep{sarkka2013bayesian} and thus the name Particle-Filter for the
sequential importance resample algorithm. In the following we present
two examples of the Particle-Filter at work.

\hypertarget{particle-filter-example}{%
\subsection{Particle-Filter Example}\label{particle-filter-example}}

For the first example we generate data according to:

\[
\begin{equation*}
    \begin{aligned}
        x_t  &= 0.5*x_{t-1} + 25*x_{t-1}/(1 + x_{t-1}^2) + 8*cos(1.2*(t-1)) + \varepsilon_{t-1},\ \varepsilon_{t-1} \sim \mathcal{N}(0,1)\\
        y_t  &= x_t^2/20 + \nu_t, \ \nu_t \sim \mathcal{N}(0,1)
    \end{aligned}
\end{equation*}
\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# particle filter simulatin example 1}

\CommentTok{# init state}
\NormalTok{x <-}\StringTok{ }\FloatTok{0.1}
\CommentTok{# process noise}
\NormalTok{x_N <-}\StringTok{ }\DecValTok{1} 
\CommentTok{# measurement noise}
\NormalTok{x_R <-}\StringTok{ }\DecValTok{1}
\CommentTok{# duration}
\NormalTok{T <-}\StringTok{ }\DecValTok{100}
\CommentTok{# num particles}
\NormalTok{N <-}\StringTok{ }\DecValTok{1000}
\CommentTok{# init var}
\NormalTok{V <-}\StringTok{ }\DecValTok{2}

\CommentTok{# save variables}
\NormalTok{z_out <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(T)}
\NormalTok{x_out <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(T)}
\NormalTok{x_est <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(T)}
\NormalTok{x_est_out <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(T)}
\NormalTok{weight <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(N)}


\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\CommentTok{# init particles}
\NormalTok{particles <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(N, x, }\KeywordTok{sqrt}\NormalTok{(V))}

\ControlFlowTok{for}\NormalTok{(t }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{T)\{}
\NormalTok{    x  <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{state_update}\NormalTok{(x,t}\DecValTok{-1}\NormalTok{), x_N)}
\NormalTok{    z <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{measurement_update}\NormalTok{(x), }\DecValTok{2}\NormalTok{)}

    \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{N)\{}
\NormalTok{        particles[i] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{state_update}\NormalTok{(particles[i],t}\DecValTok{-1}\NormalTok{), x_N)}
\NormalTok{        z_update <-}\StringTok{ }\KeywordTok{measurement_update}\NormalTok{(particles[i])}
\NormalTok{        weight[i] <-}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(z, z_update, }\DecValTok{2}\NormalTok{)}
\NormalTok{    \}}
\NormalTok{    weight  <-}\StringTok{ }\NormalTok{weight }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(weight)}
\NormalTok{    particles <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(particles, }\DataTypeTok{size=}\NormalTok{N, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{prob=}\NormalTok{weight)}
\NormalTok{    x_est <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(particles)}
\NormalTok{    x_out[t] <-}\StringTok{ }\NormalTok{x}
\NormalTok{    z_out[t] <-}\StringTok{ }\NormalTok{z}
\NormalTok{    x_est_out[t] <-}\StringTok{ }\NormalTok{x_est}
\NormalTok{\}}

\KeywordTok{plot}\NormalTok{(x_out, }\DataTypeTok{type=}\StringTok{'l'}\NormalTok{, }\DataTypeTok{main=}\StringTok{"State Developement"}\NormalTok{, }\DataTypeTok{xlab=} \StringTok{"Iteration"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"State"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x_est_out, }\DataTypeTok{col=}\StringTok{'red'}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{Kalman_F_files/figure-latex/unnamed-chunk-9-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(z_out, }\DataTypeTok{type=}\StringTok{'l'}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Position Developement"}\NormalTok{, }\DataTypeTok{xlab=} \StringTok{"Iteration"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Position"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{measurement_update}\NormalTok{(x_est_out), }\DataTypeTok{col=}\StringTok{'red'}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.4\linewidth]{Kalman_F_files/figure-latex/unnamed-chunk-9-2} \end{center}

For the second example we try to estimate the movement of the volatility
index:

\[
\begin{equation*}
    \begin{aligned}
        x_t  &=  0.69 + x_{t-1} + \varepsilon_{t-1},\ \varepsilon_{t-1} \sim \mathcal{N}(0,1.12)\\
        y_t  &= exp(x_t) + \nu_t, \ \nu_t \sim \mathcal{N}(0,0.78)
    \end{aligned}
\end{equation*}
\]

The data is obtained from the Federal Reserve Bank of St.~Louis.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# exaple stochastic volatility model}

\CommentTok{# read data}
\NormalTok{vix_orig <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"./VIXCLS.csv"}\NormalTok{)}

\CommentTok{# inspect data}
\CommentTok{# dim(vix_orig)}
\CommentTok{# head(vix_orig)}
\CommentTok{# tail(vix_orig)}

\CommentTok{# missing values?}
\NormalTok{vix <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(vix_orig[,}\DecValTok{2}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: NAs introduced by coercion
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# remove missing values}
\NormalTok{vix <-}\StringTok{ }\NormalTok{vix[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(vix)]}

\CommentTok{# init variables}
\NormalTok{n <-}\StringTok{ }\DecValTok{1000}
\NormalTok{T  <-}\StringTok{ }\KeywordTok{length}\NormalTok{(vix)}

\CommentTok{# model parameters}
\NormalTok{a <-}\StringTok{ }\FloatTok{0.69}
\NormalTok{b <-}\StringTok{ }\DecValTok{1}
\NormalTok{sig_v <-}\StringTok{ }\FloatTok{1.12}
\NormalTok{B <-}\StringTok{ }\FloatTok{0.89}
\NormalTok{sig_w <-}\StringTok{ }\FloatTok{0.78}

\CommentTok{# save estimate}
\NormalTok{x_est_out <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(T)}
\CommentTok{# state <-numeric(T)}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\CommentTok{# generate particles}
\NormalTok{particles <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd =}\NormalTok{ sig_v)}

\CommentTok{# perform calculations}
\ControlFlowTok{for}\NormalTok{(t }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{T)\{}
\NormalTok{    particles <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{mean=}\KeywordTok{stochastic_volatility_state}\NormalTok{(particles, a, b), }\DataTypeTok{sd =} \KeywordTok{sqrt}\NormalTok{(sig_v))}
\NormalTok{    z_update <-}\StringTok{ }\KeywordTok{stochastic_volatility_process}\NormalTok{(particles)}
\NormalTok{    weight <-}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(vix[t], z_update, sig_w)}
    \CommentTok{# weight <- weight * dnorm(z[t], z_update, sig_w)}
\NormalTok{    weight  <-}\StringTok{ }\NormalTok{weight }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(weight)}
\NormalTok{    particles <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(particles, }\DataTypeTok{size=}\NormalTok{n, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{prob=}\NormalTok{weight)}
\NormalTok{    x_est <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(particles)}
\NormalTok{    x_est_out[t] <-}\StringTok{ }\NormalTok{x_est}
\NormalTok{\}}

\CommentTok{# save results}

\KeywordTok{plot}\NormalTok{(vix, }\DataTypeTok{type=}\StringTok{'l'}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Vix"}\NormalTok{, }\DataTypeTok{xlab=} \StringTok{"Time"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Value"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{stochastic_volatility_process}\NormalTok{(x_est_out), }\DataTypeTok{col=}\StringTok{'red'}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Kalman_F_files/figure-latex/unnamed-chunk-10-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(vix[}\DecValTok{1}\OperatorTok{:}\DecValTok{50}\NormalTok{], }\DataTypeTok{type=}\StringTok{'l'}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Vix First 50 Observations"}\NormalTok{, }\DataTypeTok{xlab=} \StringTok{"Time"}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{"Value"}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(}\KeywordTok{stochastic_volatility_process}\NormalTok{(x_est_out)[}\DecValTok{1}\OperatorTok{:}\DecValTok{50}\NormalTok{], }\DataTypeTok{col=}\StringTok{'red'}\NormalTok{, }\DataTypeTok{lty=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Kalman_F_files/figure-latex/unnamed-chunk-10-2} \end{center}

These two figures depicts the results of the Vix estimation for the
entire series. It looks like the original and the measurement are
identical. The upper figure presents the original data and estimation
for the first 50 observations. As we can see, they are not identical,
but the model captures the movement of the observations very well.

\hypertarget{conclusio}{%
\section{Conclusio}\label{conclusio}}

In this work we presented an overview of Bayesian filtering techniques
by describing the Kalman- and Extended-Kalman-Filter as well as the
Particle-Filter with simulation studies for each model. For gaussian
models we have seen that there exists a closed- or approximately closed
form solution (Kalman-, Extended-Kalman-Filter respectively). Dropping
the assumption of the gaussian distribution lead to a more flexible
framework that can handle different distributional forms by relying on
Monte Carlo techniques.

\end{document}
